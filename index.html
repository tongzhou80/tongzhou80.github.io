<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8">
    <meta name="google-site-verification" content="ggSi8u92Ra7SlBINECfoH33kr6itXN12m-koSaOQjzc" />
    <title>Tong Zhou</title>
    <style>
      body {
      width: 61.8%;
      margin: 40px auto;
      color: #333333;
      <!-- font-family: "Lato", "Helvetica Neue", "Georgia", "Calibri"; -->
      font-size: 11pt;
      line-height: 1.42857143;
      }

      .project_title {
      font-weight: bold;
      margin: 15px auto;
      }

      /* input:focus { */
      /* 	  outline:none; */
      /* } */

    </style>
  </head>
  <body>
    <h1>Tong Zhou</h1>
    <img src="pics/TongZhou.jpg" alt="08/2015" height="210">

    <h2>About Me</h2>
    <p>
      I received my PhD in Computer Science from Georgia Tech, working with the <a href="https://habanero.cc.gatech.edu/">Habanero</a> folks on compiler optimizations and parallel computing. My work has focused on building programming tools and compiler techniques that improve code performance and programming productivity, especially for domains such as scientific computing and machine learning. 

 <!-- to make code run faster. In particular I enjoy looking at problems from both algorithmic complexity and efficient implementation perspective. -->
    </p>

    <p>
      Please feel free to send me an email (<a href="mailto:tz@gatech.edu">tz@gatech.edu</a>) if you are interested in code optimitization or you have some code that you hope to run faster. For anyone who's insterested in learning more about high performance computing, I highly recommend these <a href="hpc-study-materials.html">materials</a>.
    </p>

    <p>My Curriculum Vitae is available <a href="resume_2024_May.pdf">here</a>. </p>

<!-- on programming language and software system. My research aims to improve correctness and performance of softwares by making better compiler and runtime systems. -->
    

    <h2>Publications</h2>
    <ul>
      <li>
        <b>High-Level Compiler Optimizations for Python Programs.</b><br>
        Tong Zhou.<br>
        <i>PhD Dissertation. April 2024.</i><br>
        [<a href="#" title="show/unshow the abstract" onclick="showAbstract('dissertation'); return false;">Abstract</a>] [<a href="https://repository.gatech.edu/bitstreams/d486c3a9-c621-4151-b44f-43d71daa194e/download">PDF</a>] [<a href="talks/dissertation.pdf">Slides</a>]
      </li>
      <p id="ab-dissertation" filled=""></p>
      
      <li>
        <b>APPy: Annotated Parallelism for Python on GPUs.</b><br>
        Tong Zhou, Jun Shirako, Vivek Sarkar.<br>
        <i>Proceedings of the 33rd ACM SIGPLAN International Conference on Compiler Construction (CC 2024). March 2024.</i><br>
        [<a href="#" title="show/unshow the abstract" onclick="showAbstract('appy'); return false;">Abstract</a>] [<a href="https://dl.acm.org/doi/pdf/10.1145/3640537.3641575">PDF</a>] [<a href="talks/APPy.pdf">Slides</a>] [<a href="https://github.com/habanero-lab/APPy">GitHub</a>] [<a href="https://codefile.io/f/CLS1EqdcwA">Demo</a>]
      </li>
      <p id="ab-appy" filled=""></p>
      <!-- <input type="text" id="comment-appy" style="display:none;"> -->

      <li>
        <b>ReACT: <u>Re</u>dundancy-<u>A</u>ware <u>C</u>ode Generation for <u>T</u>ensor Expressions.</b><br>
        Tong Zhou, Ruiqin Tian, Rizwan A Ashraf, Roberto Gioiosa, Gokcen Kestor, Vivek Sarkar.<br>
        <i>Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT 2022). October 2022.</i><br>
        [<a href="#" title="show/unshow the abstract" onclick="showAbstract('react'); return false;">Abstract</a>] [<a href="https://dl.acm.org/doi/pdf/10.1145/3559009.3569685">PDF</a>] [<a href="talks/ReACT-thesis-version.pdf">Slides</a>] [<a href="https://codefile.io/f/RswyR8rIAp">Demo</a>]
      </li>
      <p id="ab-react"></p>

      <li>
        <b>Intrepydd: Performance, Productivity, and Portability for Data Science Application Kernels.</b><br>
        Tong Zhou, Jun Shirako, Anirudh Jain, Sriseshan Srikanth, Thomas M Conte, Richard Vuduc, Vivek Sarkar.<br>
        <i>Proceedings of the 2020 ACM SIGPLAN International Symposium on New Ideas, New Paradigms, and Reflections on Programming and Software (Onward! 2020). November 2020.</i><br>
        [<a href="#" title="show/unshow the abstract" onclick="showAbstract('intrepydd'); return false;">Abstract</a>] [<a href="https://dl.acm.org/doi/pdf/10.1145/3426428.3426915">PDF</a>] [<a href="talks/Intrepydd-thesis-version.pdf">Slides</a>] [<a href="https://codefile.io/f/Zb0QWSasbX">Demo</a>]
      </li>
      <p id="ab-intrepydd"></p>

      <li>
        <b>Valence: Variable Length Calling Context Encoding.</b><br>
        Tong Zhou, Michael R. Jantz, Prasad A. Kulkarni, Kshitij A. Doshi, and Vivek Sarkar.<br>
        <i>28th ACM SIGPLAN International Conference on Compiler Construction (CC 2019). February 2019. </i><br>
        [<a href="#" title="show/unshow the abstract" onclick="showAbstract('valence'); return false;">Abstract</a>] [<a href="pubs/cc19.pdf">PDF</a>] [<a href="talks/valence.pdf">Slides</a>] [<a href="https://gitlab.com/tongzhou/valence">GitHub</a>]
      </li>
      <p id="ab-valence"></p>

      <li>
        <b>MemBrain: Automated Application Guidance for Hybrid Memory Systems.</b><br>
        M. Benjamin Olson, Tong Zhou, Michael R. Jantz, Kshitij A. Doshi, M. Graham Lopez, and Oscar Hernandez.<br>
        <i>13th IEEE International Conference on Networking, Architecture, and Storage (NAS 2018). October 2018. (Awarded Best Paper) </i><br>
        [<a href="#" title="show/unshow the abstract" onclick="showAbstract('membrain'); return false;">Abstract</a>]  [<a href="https://web.eecs.utk.edu/~mrjantz/papers/membrain-automated-application.pdf">PDF</a>]
      </li>
      <p id="ab-membrain"></p>

      <li>
        <b>On Automated Feedback-Driven Data Placement in Multi-tiered Memory.</b><br>
        T. Chad Effler, Adam P. Howard, Tong Zhou, Michael R. Jantz, Kshitij A. Doshi, Prasad A. Kulkarni.<br>
        <i>Proceedings of the 31st International Conference on Architecture of Computing Systems, ARCS 2018.</i><br>
        [<a href="#" title="show/unshow the abstract" onclick="showAbstract('arcs18'); return false;">Abstract</a>]  [<a href="https://web.eecs.utk.edu/~mrjantz/papers/arcs18.pdf">PDF</a>]
      </li>
      <p id="ab-arcs18"></p>
    </ul>

<script>
let abs = {
  "valence": `
Many applications, including program optimizations, debugging tools, and event loggers, rely on calling context to gain additional insight about how a program behaves during execution. One common strategy for determining calling contexts is to use compiler instrumentation at each function call site and return sites to encode the call paths and store them in a designated area of memory. While recent works have shown that this approach can generate precise calling context encodings with low overhead, the encodings can grow to hundreds or even thousands of bytes to encode a long call path, for some applications. Such lengthy encodings increase the costs associated with storing, detecting, and decoding call path contexts, and can limit the effectiveness of this approach for many usage scenarios.
This work introduces a new compiler-based strategy that significantly reduces the length of calling context encoding with little or no impact on instrumentation costs for many applications. Rather than update or store an entire word at each function call and return, our approach leverages static analysis and variable length instrumentation to record each piece of the calling context using only a small number of bits, in most cases. We implemented our approach as an LLVM compiler pass, and compared it directly to the state-of-the-art calling context encoding strategy (PCCE) using a standard set of C/C++ applications from SPEC CPU 2017. Overall, our approach reduces the length of calling context encoding from 4.3 words to 1.6 words on average (> 60% reduction), thereby improving the efficiency of applications that frequently store or query calling contexts.
`,
  "membrain": `
Computer systems with multiple tiers of memory devices with different latency, bandwidth, and capacity characteristics are quickly becoming mainstream. Due to cost and physical limitations, device tiers that enable better performance typically include less capacity. Such heterogeneous memory systems require alternative data management strategies to utilize the capacity-constrained resources efficiently. However, current techniques are often limited because they rely on inflexible hardware caching or manual modifications to source code. This paper introduces MemBrain, a new memory management framework that automates the production and use of data-tiering guidance for applications on hybrid memory systems. MemBrain employs program profiling and source code analysis to enable transparent and efficient data placement across different types of memory. It automatically clusters data with similar expected usage patterns into page-aligned regions of virtual addresses (arenas), and uses offline profile feedback to direct low-level tier assignments for each region. We evaluate MemBrain on an Intel Knights Landing server machine with an upper tier of limited capacity (but higher bandwidth) MCDRAM and a lower tier of conventional DDR4 using a selection of high-bandwidth benchmarks from SPEC CPU 2017 as well as two proxy apps (Lulesh and AMG), and one full scale scientific application (QMCPACK). Our evaluation shows that MemBrain can achieve significant performance and efficiency improvements compared to current guided and unguided management strategies.
`,

  "intrepydd": `Major simultaneous disruptions are currently under way in both hardware and software. In hardware, extreme heterogeneity has become critical to sustaining cost and performance improvements after Moore's Law, but poses productivity and portability challenges for developers. In software, the rise of large-scale data science is driven by developers who come from diverse backgrounds and, moreover, who demand the rapid prototyping and interactive-notebook capabilities of high-productivity languages like Python.

We introduce the Intrepydd programming system, which enables data scientists to write application kernels with high performance, productivity, and portability on current and future hardware. Intrepydd is based on Python, though the approach can be applied to other base languages as well. To deliver high performance, the Intrepydd toolchain uses ahead-of-time (AOT) compilation and high-level compiler optimizations of Intrepydd kernels. Intrepydd achieves portability by its ability to compile kernels for execution on different hardware platforms, and for invocation from Python or C++ main programs.

An empirical evaluation shows significant performance improvements relative to Python, and the suitability of Intrepydd for mapping on to post-Moore accelerators and architectures with relative ease. We believe that Intrepydd represents a new direction of \"Discipline-Aware Languages\" (DiALs), which brings us closer to the holy grail of obtaining productivity and portability with higher performance than current Python-like languages, and with more generality than current domain-specific languages and libraries. <br><img src="pics/DiALs.jpg" alt="Discipline-Aware Languages" width="400" height="auto">`,

  "react": `High-level programming models for tensor computations are becoming increasingly popular in many domains such as machine learning and data science. The index notation is one such model that is widely adopted for expressing a wide range of tensor computations algorithmically and also as input to programming systems. In programming systems, sparse tensors can be specified as type annotations, and a compiler can be employed to perform code generation for the specified tensor expressions and sparse formats. Different code generation strategies and optimization decisions can have a significant impact on the performance of the generated code. However, the code generation strategies used by current state-of-the-art tensor compilers can result in redundant computations being present in the output code. In this work, we identify four common types of redundancies that can occur when generating code for compound expressions, and introduce new techniques that can avoid these redundancies. Empirical evaluation on real-world compound kernels, such as Sampled Dense Dense Matrix Multiplication (SDDMM), Graph Neural Network (GNN) and Matricized-Tensor Times Khatri-Rao Product (MTTKRP) shows that our generated code with redundancy elimination can result in performance improvements of 1.1× to 25× relative to a state-of-the-art Tensor Algebra COmpiler (TACO) and up to 101× relative to library approaches such as the SciPy.sparse.`,

    "appy": `GPUs are increasingly being used to speed up Python applications in the scientific computing and machine learning domains. Currently, the two common approaches to leveraging GPU acceleration in Python are 1) create a custom native GPU kernel, and import it as a function that can be called from Python; 2) use libraries such as CuPy, which provides pre-defined GPU-implementation-backed tensor operators. The first approach is very flexible but requires tremendous manual effort to create a correct and high performance GPU kernel. While the second approach dramatically improves productivity, it is limited in its generality, as many applications cannot be expressed purely using CuPy’s pre-defined tensor operators. Additionally, redundant memory access can often occur between adjacent tensor operators due to the materialization of intermediate results. In this work, we present APPy (Annotated Parallelism for Python), which enables users to parallelize generic Python loops and tensor expressions for execution on GPUs by adding simple compiler directives (annotations) to Python code. Empirical evaluation on 20 scientific computing kernels from the literature on a server with an AMD Ryzen 7 5800X 8-Core CPU and an NVIDIA RTX 3090 GPU demonstrates that with simple pragmas APPy is able to generate more efficient GPU code and achieves significant geometric mean speedup relative to CuPy (30× on average), and to three state-of-the-art Python compilers, Numba (8.3× on average), DaCe-GPU (3.1× on average) and JAX-GPU (18.8× on average).`,

    "arcs18": `Recent emergence of systems with multiple performance and capacity tiers of memory invites a fresh consideration of strategies for optimal placement of data into the various tiers. This work explores a variety of cross-layer strategies for managing application data in multitiered memory. We propose new profiling techniques based on the automatic classification of program allocation sites, with the goal of using those classifications to guide memory tier assignments.We evaluate our approach with different profiling inputs and application strategies, and show that it outperforms other state-of-the-art management techniques.`,

    "dissertation": `As Python becomes the de facto high-level programming language for many data analyt- ics and scientific computing domains, it becomes increasingly critical to build optimizing compilers that are able to generate efficient sequential and parallel code from Python pro- grams to keep up with the insatiable demands for performance in these domains. Programs written in high-level languages like Python often make extensive use of arrays as a core data type, and mathematical functions applied on the arrays, in conjunction with general loops and element-level array accesses. Such a programming style poses both challenges and opportunities for optimizing compilers. We recognize that current compilers are limited in their ability to make effective use of the high-level operator and loop semantics to generate efficient code on modern parallel architectures. This dissertation presents three pieces of work that demonstrate that compilers that leverage high-level operator and loop semantics can deliver improved performance for Python programs on CPUs and GPUs, relative to past work. On the CPU front, we present Intrepydd, a Python to C++ compiler that compiles a broad class of Python language constructs and NumPy array operators to sequential and parallel C++ code on CPUs. On the GPU front, we present APPy (Annotated Parallelism for Python), which enables users to parallelize generic Python loops and tensor expressions for execution on GPUs by simply adding compiler directives (annotations) to Python code. Then for programs consisting of sparse tensor operators, we introduce ReACT, which consists of a set of code generation techniques that achieve greater redundancy elimination than state-of-the-art.`
}

function showAbstract(name) {
    tag = document.getElementById('ab-' + name) ;
    // if (tag.getAttribute("filled") === "demo") {
    // 	tag.setAttribute("filled", "");
    // }

    // if (tag.getAttribute("filled") === "") {
    // 	tag.innerHTML = abs[name];
    // 	tag.setAttribute("filled", "abstract");
    // }
    // else {
    // 	tag.innerHTML = "";
    // 	tag.setAttribute("filled", "");
    // }
    
  if (tag.innerHTML === "") {
      tag.innerHTML = abs[name];
      tag.style.borderStyle = "dashed";
      tag.style.borderWidth = "thin";
      tag.style.padding = "10px";
//      tag.style.textAlign = "center";
  }
  else {
      tag.innerHTML = "";
      tag.style.borderStyle = "";
      tag.style.padding = "";
  }
}

  function showAPPyDemo() {
      tag = document.getElementById('ab-appy');
      // console.log(tag.hasAttribute("filled"));
      // console.log(tag.getAttribute("filled"));
      // if (tag.innerHTML === '') {
      // 	  tag.innerHTML = abs[name];
      // }
      // else {
      // 	  tag.innerHTML = '';
      // }
  }

  function showCommentBox(name) {
      tag = document.getElementById('comment-' + name) ;
      console.log(tag);
      if (tag.style.display === "none") {
	  tag.style.display = "block";
	  tag.setAttribute("size", 100);
	  tag.style.borderStyle = "dashed";
      tag.style.borderWidth = "thin";
      tag.style.padding = "10px";
      } else {
	  tag.style.display = "none";
      }
  }
</script>

    <!-- <h2>Teachings</h2> -->
    <!-- <p><a href="./classes/cs581/index.html">CS 581 Algorithms</a></p> -->

    <!-- <h2>Recommended Readings</h2> -->
    <!-- <p><a href="readings.html">Recommended readings</a></p> -->
    
    <h2>Contact</h2>
    <ul>
      <li>tz at gatech edu</li>
      <li>2337 Klaus Advanced Computing Building,
        266 Ferst Dr NW,
        Atlanta, GA 30332
      </li>
    </ul>
    <!-- <h2>Life</h2> -->
    <!-- <p>TBD</p> -->
  </body>
</html>
